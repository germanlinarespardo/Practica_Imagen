{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aca1fba",
   "metadata": {
    "papermill": {
     "duration": 0.019113,
     "end_time": "2021-12-08T11:27:11.446782",
     "exception": false,
     "start_time": "2021-12-08T11:27:11.427669",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22a504a",
   "metadata": {
    "papermill": {
     "duration": 0.019093,
     "end_time": "2021-12-08T11:27:11.485405",
     "exception": false,
     "start_time": "2021-12-08T11:27:11.466312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Neural style transfer\n",
    "\n",
    "### Convolutional Neural Network:\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a category of Neural Network that have proven very effective in areas such as image recognition and classification. CNNs have been successful in computer vision related problems like identifying faces, objects and traffic signs apart from powering vision in robots and self driving cars.\n",
    "\n",
    "CNN is shown to be able to well replicate and optimize these key steps in a unified framework and learn hierarchical representations directly from raw images. If we take a convolutional neural network that has already been trained to recognize objects within images then that network will have developed some internal independent representations of the content and style contained within a given image.\n",
    "\n",
    "In 2014, the winner of the ImageNet challenge was a network created by Visual Geometry Group (VGG) at Oxford University, as a basis for trying to extract content and style representations from images, naming it after them.\n",
    "\n",
    "The VGG net where shallow layers learns low level features and as we go deeper into the network these convolutional layers are able to represent much larger scale features and thus have a higher-level representation of the image content.\n",
    "\n",
    "Neural style transfer can be implemented using any pre-trained convnet. Here we will use the VGG19 and VGG16 networks. VGG19 is a simple variant of the VGG16 network, with three more convolutional layers.\n",
    "\n",
    "Using multiple convolution layers with smaller convolution kernels instead of a larger convolution layer with convolution kernels can reduce parameters on the one hand, and the author believes that it is equivalent to more non-linear mapping, which increases the Fit expression ability.\n",
    "\n",
    "For the VGG16 network, every step they applied kernals for 2–3 time and then applied a max pooling layer. In our case, we want to account for features across the entire image so we get rid of the maxpool which throws away information and replace those layers for ones that compute the Average Pooling instead. Each time the number of kernals are doubled from the previous layer meaning that each time we trying to extract more and more features. At the end the of the network three fully connected layers are used to limit the relu activation function grow. Dropout is also implemented for reduce overfitting of model.\n",
    "\n",
    " ![VGG16](/ImagenesNotebook/vgg16-architecture.png)\n",
    "\n",
    "Regarding the VGG19 network, it has 16 convolutions with ReLUs between them and five maxpooling layers which we will also substitute for the Average Pooling. The number of filter maps of the convolutions start at 64 and grow until 512. After the convolutions, there is a linear classifier made-up three fully-connected (fc) layers with dropout (SHK * 14) between them, the first two have 4096 features while the last one has 1000. The last fc layer is connected to a softmax which maps each value to the probabilities of belonging to each of the 1000 classes of the ImageNet competition. \n",
    "\n",
    " ![VGG19](/ImagenesNotebook/vgg19-architecture.png)\n",
    "\n",
    "### Style Transfer\n",
    "\n",
    "Style Transfer is a technique of modifying one image in style of another image. We are implementing Gatys style transfer which was originally released in 2015 by Gatys et al. The neural style transfer algorithm has undergone many refinements and spawned many variations. Neural style transfer consists in applying the \"style\" of a reference image to a target image, while conserving the \"content\" of the target image:\n",
    "\n",
    "Style refers to the textures, colors, and visual patterns in an image while the \"content\" is the higher-level macrostructure of the image. \n",
    "\n",
    "The key point behind style transfer is same idea that is core to all deep learning algorithms: we define a loss function to specify what we want to achieve, and we minimize this loss. We want to achieve: conserve the \"content\" of the original image, while adopting the \"style\" of the reference image. The theoretical loss function would be the following:\n",
    "\n",
    "We can construct images whose feature maps at a chosen convolution layer match the corresponding feature maps of a given content image. We expect the two images to contain the same content — but not necessarily the same texture and style.\n",
    "\n",
    "#### Loss\n",
    "\n",
    "##### The content loss\n",
    "\n",
    "Given a chosen content layer l, the content loss is defined as the Mean Squared Error between the feature map F of our content image C and the feature map P of our generated image Y.\n",
    "\n",
    "When this content-loss is minimized, it means that the mixed-image has feature activation in the given layers that are very similar to the activation of the content-image. Depending on which layers we select, this should transfer the contours from the content-image to the mixed-image.\n",
    "\n",
    "As you already know, *activations from earlier layers in a network contain local information about the image*, while *activations from higher layers contain increasingly global and abstract information*. Therefore we expect the \"content\" of an image, which is more global and more abstract, to be captured by the representations of a top layer of a convnet.\n",
    "\n",
    "##### The style loss\n",
    "\n",
    "Now we want to measure which features in the style-layers activate simultaneously for the style-image, and then copy this activation-pattern to the mixed-image.\n",
    "\n",
    "One way of doing this, is to calculate the Gram-matrix(a matrix comprising of correlated features) for the tensors output by the style-layers. The Gram-matrix is essentially just a matrix of dot-products for the vectors of the feature activations of a style-layer. This inner product can be understood as representing a map of the correlations between the features of a layer. These feature correlations capture the statistics of the patterns of a particular spatial scale, which empirically corresponds to the appearance of the textures found at this scale. If an entry in the Gram-matrix has a value close to zero then it means the two features in the given layer do not activate simultaneously for the given style-image. And vice versa, if an entry in the Gram-matrix has a large value, then it means the two features do activate simultaneously for the given style-image. We will then try and create a mixed-image that replicates this activation pattern of the style-image.\n",
    "\n",
    "Hence the style loss aims at preserving similar internal correlations within the activations of different layers, across the style reference image and the generated image. In turn, this guarantees that the textures found at different spatial scales will look similar across the style reference image and the generated image. The loss function for style is quite similar to out content loss, except that we calculate the Mean Squared Error for the Gram-matrices instead of the raw tensor-outputs from the layers.\n",
    "\n",
    "### TL:DR\n",
    "\n",
    "In short, being the content image the one we wish to modify and the style reference image the one we obtain the style from, we can use a pre-trained convnet to define a loss that will:\n",
    "\n",
    "* Preserve content by maintaining similar high-level layer activations between the target content image and the generated image. The convnet should \"see\" both the target image and the generated image as \"containing the same things\".\n",
    "* Preserve style by maintaining similar correlations within activations for both low-level layers and high-level layers. Indeed, feature correlations capture textures: the generated and the style reference image should share the same textures at different spatial scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ca2e9c5a",
   "metadata": {
    "_cell_guid": "bd0a0c10-6341-43dc-8003-67666d32ab54",
    "_uuid": "b67fe233-4c51-4117-b53c-52464898b24d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.423056,
     "end_time": "2021-12-08T11:27:16.927579",
     "exception": false,
     "start_time": "2021-12-08T11:27:11.504523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "\n",
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from skimage.transform import resize\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from keras.preprocessing.image import save_img\n",
    "\n",
    "import time\n",
    "import imageio\n",
    "from PIL import Image\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f981b88e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b30e552a",
   "metadata": {
    "_cell_guid": "bd0a0c10-6341-43dc-8003-67666d32ab54",
    "_uuid": "b67fe233-4c51-4117-b53c-52464898b24d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026308,
     "end_time": "2021-12-08T11:27:17.148747",
     "exception": false,
     "start_time": "2021-12-08T11:27:17.122439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_img_and_preprocess(path, shape=None):\n",
    "    img = image.load_img(path, target_size=shape)\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fe090923",
   "metadata": {
    "_cell_guid": "bd0a0c10-6341-43dc-8003-67666d32ab54",
    "_uuid": "b67fe233-4c51-4117-b53c-52464898b24d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.025144,
     "end_time": "2021-12-08T11:27:17.020917",
     "exception": false,
     "start_time": "2021-12-08T11:27:16.995773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unpreprocess(img):\n",
    "    # Remove zero-center by mean pixel\n",
    "    img[..., 0] += 103.939\n",
    "    img[..., 1] += 116.779\n",
    "    img[..., 2] += 126.68\n",
    "    # 'BGR'->'RGB'\n",
    "    img = img[:, :, ::-1]\n",
    "    img = np.clip(img, 0, 255).astype('uint8')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "785b1d1a",
   "metadata": {
    "_cell_guid": "bd0a0c10-6341-43dc-8003-67666d32ab54",
    "_uuid": "b67fe233-4c51-4117-b53c-52464898b24d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.028844,
     "end_time": "2021-12-08T11:27:16.976773",
     "exception": false,
     "start_time": "2021-12-08T11:27:16.947929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def VGG16_AvgPool(shape):\n",
    "    vgg = VGG16(input_shape=shape, weights='imagenet', include_top=False)\n",
    "    i = vgg.input\n",
    "    x = i\n",
    "    for layer in vgg.layers:\n",
    "        if layer.__class__ == MaxPooling2D:\n",
    "        # replace it with average pooling\n",
    "            x = AveragePooling2D()(x)\n",
    "        else:\n",
    "            x = layer(x)\n",
    "\n",
    "    return Model(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4dc41a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG19_AvgPool(shape):\n",
    "    vgg = VGG19(input_shape=shape, weights='imagenet', include_top=False)\n",
    "    i = vgg.input\n",
    "    x = i\n",
    "    for layer in vgg.layers:\n",
    "        if layer.__class__ == MaxPooling2D:\n",
    "        # replace it with average pooling\n",
    "            x = AveragePooling2D()(x)\n",
    "        else:\n",
    "            x = layer(x)\n",
    "\n",
    "    return Model(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9e5e8380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG16_AvgPool_concatenate(tensor):\n",
    "  # we want to account for features across the entire image\n",
    "  # so get rid of the maxpool which throws away information\n",
    "    vgg = VGG16(input_tensor=tensor, weights='imagenet', include_top=False)\n",
    "    i = vgg.input\n",
    "    x = i\n",
    "    for layer in vgg.layers:\n",
    "        if layer.__class__ == MaxPooling2D:\n",
    "        # replace it with average pooling\n",
    "            x = AveragePooling2D()(x)\n",
    "        else:\n",
    "            x = layer(x)\n",
    "\n",
    "    return Model(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2522a637-1b08-47a7-b72b-0d402d1a4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG19_AvgPool_concatenate(tensor):\n",
    "  # we want to account for features across the entire image\n",
    "  # so get rid of the maxpool which throws away information\n",
    "    vgg = VGG19(input_tensor=tensor, weights='imagenet', include_top=False)\n",
    "    i = vgg.input\n",
    "    x = i\n",
    "    for layer in vgg.layers:\n",
    "        if layer.__class__ == MaxPooling2D:\n",
    "        # replace it with average pooling\n",
    "            x = AveragePooling2D()(x)\n",
    "        else:\n",
    "            x = layer(x)\n",
    "\n",
    "    return Model(i, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d97afa4-b2ba-4ecb-9202-47ed2271e2d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b1591511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aaceba39",
   "metadata": {
    "papermill": {
     "duration": 0.026529,
     "end_time": "2021-12-08T11:27:17.194433",
     "exception": false,
     "start_time": "2021-12-08T11:27:17.167904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gram_matrix(img):\n",
    "    # input is (H, W, C) (C = # feature maps)\n",
    "    # we first need to convert it to (C, H*W)\n",
    "    X = K.batch_flatten(K.permute_dimensions(img, (2, 0, 1)))\n",
    "\n",
    "    # now, calculate the gram matrix\n",
    "    # gram = XX^T / N\n",
    "    # the constant is not important since we'll be weighting these\n",
    "    G = K.dot(X, K.transpose(X)) / img.get_shape().num_elements()\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "caabe09b",
   "metadata": {
    "papermill": {
     "duration": 0.028242,
     "end_time": "2021-12-08T11:27:17.242325",
     "exception": false,
     "start_time": "2021-12-08T11:27:17.214083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = h * w\n",
    "    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    a = K.square(\n",
    "        x[:, :h - 1, :w - 1, :] - x[:, 1:, :w - 1, :])\n",
    "    b = K.square(\n",
    "        x[:, :h - 1, :w - 1, :] - x[:, :h - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))\n",
    "\n",
    "\n",
    "def minimize(epochs, batch_shape):\n",
    "    path = 'C:/Users/USER/Desktop/Comillas/Análisis datos no estructurados/IMAGEN/Neural-Transfer/'\n",
    "    results_folder = 'Results/'\n",
    "    results_prefix = 'style_transfer_result'\n",
    "    t0 = datetime.now()\n",
    "    losses = []\n",
    "    x = content_img\n",
    "    x = x.flatten()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        print('Start of iteration', i)\n",
    "        start_time = time.time()\n",
    "        x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x,\n",
    "                                     fprime=evaluator.grads, maxfun=5)\n",
    "        print('Current loss value:', min_val)\n",
    "        losses.append(min_val)\n",
    "        # Save current generated image\n",
    "        img = x.copy().reshape((h, w, 3))\n",
    "        img = unpreprocess(img)\n",
    "        fname = path + results_folder + results_prefix + '_at_iteration_%d.png' % i\n",
    "        imageio.imwrite(fname, img)\n",
    "        end_time = time.time()\n",
    "        print('Image saved as', fname)\n",
    "        print('Iteration %d completed in %ds' % (i, end_time - start_time))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c55b6-23f4-45a3-bf05-808646503aa7",
   "metadata": {},
   "source": [
    "### Scale Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bc8fddfa",
   "metadata": {
    "_cell_guid": "bd0a0c10-6341-43dc-8003-67666d32ab54",
    "_uuid": "b67fe233-4c51-4117-b53c-52464898b24d",
    "papermill": {
     "duration": 0.025883,
     "end_time": "2021-12-08T11:27:17.065636",
     "exception": false,
     "start_time": "2021-12-08T11:27:17.039753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scale_img(x):\n",
    "    x = x - x.min()\n",
    "    x = x / x.max()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df9b118",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fc334356",
   "metadata": {
    "papermill": {
     "duration": 0.024817,
     "end_time": "2021-12-08T11:27:17.286382",
     "exception": false,
     "start_time": "2021-12-08T11:27:17.261565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# folder names with content and style images\n",
    "style_path = 'StyleImages'\n",
    "content_path = 'ContentImages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c630b7f9",
   "metadata": {
    "papermill": {
     "duration": 0.039315,
     "end_time": "2021-12-08T11:27:17.344723",
     "exception": false,
     "start_time": "2021-12-08T11:27:17.305408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to get multiple files\n",
    "style_files = glob(style_path + '/*.jp*g')\n",
    "content_files = glob(content_path + '/*.jp*g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "603580e2",
   "metadata": {
    "papermill": {
     "duration": 0.026652,
     "end_time": "2021-12-08T11:27:17.390440",
     "exception": false,
     "start_time": "2021-12-08T11:27:17.363788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cont_img = np.random.choice(content_files)\n",
    "sty_img = np.random.choice(style_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9a812bd3",
   "metadata": {
    "_cell_guid": "bd0a0c10-6341-43dc-8003-67666d32ab54",
    "_uuid": "b67fe233-4c51-4117-b53c-52464898b24d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.048416,
     "end_time": "2021-12-08T11:27:17.458177",
     "exception": false,
     "start_time": "2021-12-08T11:27:17.409761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load content image\n",
    "content_img = load_img_and_preprocess(cont_img)\n",
    "\n",
    "# height and width for the generated picture\n",
    "h, w = content_img.shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e4cd1159",
   "metadata": {
    "_cell_guid": "bd0a0c10-6341-43dc-8003-67666d32ab54",
    "_uuid": "b67fe233-4c51-4117-b53c-52464898b24d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.14429,
     "end_time": "2021-12-08T11:27:17.659664",
     "exception": false,
     "start_time": "2021-12-08T11:27:17.515374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load style image\n",
    "style_img = load_img_and_preprocess(sty_img, (h, w))\n",
    "\n",
    "# we define batch_shape and shape of the image(?)\n",
    "batch_shape = content_img.shape\n",
    "shape = content_img.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "be9f29d2-0f42-44f7-a321-18db939f731a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_3\" was not an Input tensor, it was generated by layer \"input_4\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: Tensor(\"concat_3:0\", shape=(3, 480, 910, 3), dtype=float32)\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# content and style images are static so we use K.constant\n",
    "content_image = K.constant(content_img)\n",
    "style_image = K.constant(style_img)\n",
    "\n",
    "# placeholder that will contain our generated image\n",
    "generated_image = K.placeholder((1, h, w, 3))\n",
    "\n",
    "# we combine the 3 images into a single batch\n",
    "input_tensor = K.concatenate([content_image,\n",
    "                              style_image,\n",
    "                              generated_image], axis=0)\n",
    "\n",
    "# We build the VGG19 network with our batch of 3 images as input.\n",
    "# The model will be loaded with pre-trained ImageNet weights.\n",
    "model = VGG19_AvgPool_concatenate(input_tensor)\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e449a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict mapping layer names to activation tensors\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "# Name of layer used for content loss\n",
    "content_layer = 'block5_conv2'\n",
    "# Name of layers used for style loss\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1',\n",
    "                'block4_conv1',\n",
    "                'block5_conv1']\n",
    "# Weights in the weighted average of the loss components\n",
    "total_variation_weight = 1e-4\n",
    "style_weight = 1.\n",
    "content_weight = 0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "36d56b84-66f9-4c8b-bbd9-edf4e07a08f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss by adding all components to a `loss` variable\n",
    "loss = K.variable(0.)\n",
    "# Loss of content image\n",
    "layer_features = outputs_dict[content_layer]\n",
    "content_image_features = layer_features[0, :, :, :]\n",
    "generated_features = layer_features[2, :, :, :]\n",
    "loss = loss + (content_weight * content_loss(content_image_features,\n",
    "                                      generated_features))\n",
    "# Loss of style image\n",
    "for layer_name in style_layers:\n",
    "    layer_features = outputs_dict[layer_name]\n",
    "    style_image_features = layer_features[1, :, :, :]\n",
    "    generated_features = layer_features[2, :, :, :]\n",
    "    sl = style_loss(style_image_features, generated_features)\n",
    "    loss += (style_weight / len(style_layers)) * sl\n",
    "\n",
    "# Total loss\n",
    "loss += total_variation_weight * total_variation_loss(generated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd20505e-2df4-4513-8cfd-5e10328d18fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "54c4cb06-61c8-4b25-8398-732650f2dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we set up the gradient descent process. using the L-BFGS algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc8d203-8452-492c-9775-bb4984e57fc0",
   "metadata": {},
   "source": [
    "The L-BFGS algorithms comes packaged with SciPy. However, there are two slight limitations with the SciPy implementation:\n",
    "\n",
    "* It requires to be passed the value of the loss function and the value of the gradients as two separate functions.\n",
    "* It can only be applied to flat vectors, whereas we have a 3D image array.\n",
    "\n",
    "It would be very inefficient for us to compute the value of the loss function and the value of gradients independently, since it would lead to a lot of redundant computation between the two. We would be almost twice slower than we could be by computing them jointly. To by-pass this, we set up a Python class named Evaluator that will compute both loss value and gradients value at once, will return the loss value when called the first time, and will cache the gradients for the next call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "48df9eb6-f52a-4e51-8b1f-559d4924e978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the gradients of the generated image wrt the loss\n",
    "grads = K.gradients(loss, generated_image)[0]\n",
    "\n",
    "# Function to fetch the values of the current loss and the current gradients\n",
    "fetch_loss_and_grads = K.function([generated_image], [loss, grads])\n",
    "\n",
    "\n",
    "class Evaluator(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        x = x.reshape((1, h, w, 3))\n",
    "        outs = fetch_loss_and_grads([x])\n",
    "        loss_value = outs[0]\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "df3b1583-f416-4c83-bdb3-d923b07d14f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2a5ad517-a8cc-4b22-a855-c92acafb1fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of iteration 0\n",
      "Current loss value: 1509184.0\n",
      "Image saved as C:/Users/USER/Desktop/Comillas/Análisis datos no estructurados/IMAGEN/Neural-Transfer/Results/style_transfer_result_at_iteration_0.png\n",
      "Iteration 0 completed in 37s\n",
      "Start of iteration 1\n",
      "Current loss value: 1502255.9\n",
      "Image saved as C:/Users/USER/Desktop/Comillas/Análisis datos no estructurados/IMAGEN/Neural-Transfer/Results/style_transfer_result_at_iteration_1.png\n",
      "Iteration 1 completed in 37s\n",
      "Start of iteration 2\n",
      "Current loss value: 1497775.2\n",
      "Image saved as C:/Users/USER/Desktop/Comillas/Análisis datos no estructurados/IMAGEN/Neural-Transfer/Results/style_transfer_result_at_iteration_2.png\n",
      "Iteration 2 completed in 37s\n",
      "Start of iteration 3\n",
      "Current loss value: 1494187.2\n",
      "Image saved as C:/Users/USER/Desktop/Comillas/Análisis datos no estructurados/IMAGEN/Neural-Transfer/Results/style_transfer_result_at_iteration_3.png\n",
      "Iteration 3 completed in 37s\n",
      "Start of iteration 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [100]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m final_img \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [99]\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(epochs, batch_shape)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart of iteration\u001b[39m\u001b[38;5;124m'\u001b[39m, i)\n\u001b[0;32m     27\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 28\u001b[0m x, min_val, info \u001b[38;5;241m=\u001b[39m \u001b[43mfmin_l_bfgs_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mfprime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxfun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrent loss value:\u001b[39m\u001b[38;5;124m'\u001b[39m, min_val)\n\u001b[0;32m     31\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(min_val)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\imagen\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:199\u001b[0m, in \u001b[0;36mfmin_l_bfgs_b\u001b[1;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[0;32m    187\u001b[0m     disp \u001b[38;5;241m=\u001b[39m iprint\n\u001b[0;32m    188\u001b[0m opts \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: disp,\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miprint\u001b[39m\u001b[38;5;124m'\u001b[39m: iprint,\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxcor\u001b[39m\u001b[38;5;124m'\u001b[39m: m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m'\u001b[39m: callback,\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxls\u001b[39m\u001b[38;5;124m'\u001b[39m: maxls}\n\u001b[1;32m--> 199\u001b[0m res \u001b[38;5;241m=\u001b[39m _minimize_lbfgsb(fun, x0, args\u001b[38;5;241m=\u001b[39margs, jac\u001b[38;5;241m=\u001b[39mjac, bounds\u001b[38;5;241m=\u001b[39mbounds,\n\u001b[0;32m    200\u001b[0m                        \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopts)\n\u001b[0;32m    201\u001b[0m d \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjac\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    202\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    203\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuncalls\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnfev\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    204\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnit\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnit\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    205\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarnflag\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[0;32m    206\u001b[0m f \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\imagen\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:362\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    356\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    365\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\imagen\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\imagen\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\imagen\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\imagen\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Input \u001b[1;32mIn [89]\u001b[0m, in \u001b[0;36mEvaluator.loss\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m1\u001b[39m, h, w, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m---> 17\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_loss_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m outs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     19\u001b[0m grad_values \u001b[38;5;241m=\u001b[39m outs[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\imagen\\lib\\site-packages\\keras\\backend.py:4275\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   4269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m feed_arrays \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_arrays \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   4270\u001b[0m     symbol_vals \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol_vals \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   4271\u001b[0m     feed_symbols \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_symbols \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetches \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   4272\u001b[0m     session \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session):\n\u001b[0;32m   4273\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[1;32m-> 4275\u001b[0m fetched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marray_vals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4276\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches):])\n\u001b[0;32m   4278\u001b[0m output_structure \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[0;32m   4279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_structure,\n\u001b[0;32m   4280\u001b[0m     fetched[:\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs)],\n\u001b[0;32m   4281\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\imagen\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1480\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1479\u001b[0m   run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1480\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRunCallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1483\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m   1484\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "final_img = minimize(5, batch_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717bc88f-f046-4e9a-b824-35bc9cc98332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d8f9c-b561-4400-82d8-a8a27e295de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84b9a744",
   "metadata": {
    "_cell_guid": "bd0a0c10-6341-43dc-8003-67666d32ab54",
    "_uuid": "b67fe233-4c51-4117-b53c-52464898b24d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.029943,
     "end_time": "2021-12-08T11:27:30.370038",
     "exception": false,
     "start_time": "2021-12-08T11:27:30.340095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_loss_and_grads_wrapper(x_vec):\n",
    "    l, g = get_loss_and_grads([x_vec.reshape(*batch_shape)])\n",
    "    return l.astype(np.float64), g.flatten().astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cdde1480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_and_grads_wrapper_vgg19(x_vec):\n",
    "    l, g = get_loss_and_grads_vgg19([x_vec.reshape(*batch_shape)])\n",
    "    return l.astype(np.float64), g.flatten().astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af5c35",
   "metadata": {
    "_cell_guid": "bd0a0c10-6341-43dc-8003-67666d32ab54",
    "_uuid": "b67fe233-4c51-4117-b53c-52464898b24d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 88.347303,
     "end_time": "2021-12-08T11:28:58.740134",
     "exception": false,
     "start_time": "2021-12-08T11:27:30.392831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=0, loss=3075.550048828125\n",
      "Iteration 0 completed in 191s\n"
     ]
    }
   ],
   "source": [
    "final_img = minimize(get_loss_and_grads_wrapper, 10, batch_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22681f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_img_vgg19 = minimize(get_loss_and_grads_wrapper_vgg19, 10, batch_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8989fddd",
   "metadata": {
    "papermill": {
     "duration": 0.025258,
     "end_time": "2021-12-08T11:28:58.791330",
     "exception": false,
     "start_time": "2021-12-08T11:28:58.766072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Plot the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556b3b6",
   "metadata": {
    "papermill": {
     "duration": 0.300059,
     "end_time": "2021-12-08T11:28:59.116570",
     "exception": false,
     "start_time": "2021-12-08T11:28:58.816511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Content image\n",
    "plt.imshow(image.load_img(c_img))\n",
    "plt.figure()\n",
    "\n",
    "# Style image\n",
    "plt.imshow(image.load_img(s_img))\n",
    "plt.figure()\n",
    "\n",
    "# Generated image VGG16\n",
    "plt.imshow(scale_img(final_img))\n",
    "plt.show()\n",
    "\n",
    "# Generated image VGG16\n",
    "plt.imshow(scale_img(final_img_vgg19))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef65c40-168f-44ea-86c1-27f1693a3a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395daf5-07ab-436e-b646-aa3cf5fa33cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 119.425656,
   "end_time": "2021-12-08T11:29:03.223624",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-08T11:27:03.797968",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
